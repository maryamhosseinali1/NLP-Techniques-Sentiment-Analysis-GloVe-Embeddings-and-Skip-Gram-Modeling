{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **part1**"
      ],
      "metadata": {
        "id": "L8zdcl-BC4GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HlMx1qbJC3bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = '/content/drive/My Drive/sarcasm.json'"
      ],
      "metadata": {
        "id": "TGPWWoQPn2gx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "df = pd.read_json('/content/drive/My Drive/sarcasm.json', lines=True)\n",
        "\n",
        "# function for cleaning data\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', text)\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "# apply cleaning function to each headline\n",
        "df['cleaned_headline'] = df['headline'].apply(clean_text)\n",
        "\n",
        "# removing stopwords from cleaned headlines\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['cleaned_headline'] = df['cleaned_headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "\n",
        "# extracting labels to find if the headline is sarcastic\n",
        "y = df['is_sarcastic'].values\n",
        "\n",
        "# split the dataset into training and test sets\n",
        "train_df, eval_df, y_train, y_eval = train_test_split(df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# feature extraction with TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # maximum of 1000 features\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_headline']).toarray()  # applying TF-IDF to training data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ty6mDt15aAg",
        "outputId": "68a97a6a-c4ff-4199-a789-156d5fe537b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **part2**"
      ],
      "metadata": {
        "id": "la3mRM7bEdMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/My Drive/glove.6B.zip\" -d \"/content/drive/My Drive/glove.6B\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yndCSfS5Q34c",
        "outputId": "f96e412c-ce3d-4bc7-bb1a-0885d55fe81d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/glove.6B.zip\n",
            "replace /content/drive/My Drive/glove.6B/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n"
      ],
      "metadata": {
        "id": "wKZ8ESAXakck"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=1000)  # limit vocab to top 1000 words\n",
        "\n",
        "# learning vocab from cleaned headlines in train_df\n",
        "tokenizer.fit_on_texts(train_df['cleaned_headline'])\n",
        "\n",
        "# converting the cleaned headlines into sequences of integers\n",
        "X_train_seq = tokenizer.texts_to_sequences(train_df['cleaned_headline'])  # For training data\n",
        "X_eval_seq = tokenizer.texts_to_sequences(eval_df['cleaned_headline'])  # For evaluation data\n",
        "\n",
        "# find the maximum sequence length\n",
        "max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_eval_seq))\n",
        "\n",
        "# pad sequences\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')  # training data\n",
        "X_eval_pad = pad_sequences(X_eval_seq, maxlen=max_length, padding='post')  # evaluation data\n",
        "\n",
        "# a mapping of each word to its unique index\n",
        "word_index = tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "RhJbfNSuflgj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **part3**"
      ],
      "metadata": {
        "id": "syYTFPNGGkUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    # a dictionary to store the word embeddings.\n",
        "    embedding_index = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # split each line into word and its corresponding vector\n",
        "            values = line.split()\n",
        "            word = values[0]  # word is first value\n",
        "            # rest of values form the vector, converted to floats\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            # store the word and its vector\n",
        "            embedding_index[word] = coefs\n",
        "    return embedding_index\n",
        "# dimensions of GloVe vectors\n",
        "glove_dimensions = [50, 100, 200, 300]\n",
        "glove_paths = {\n",
        "    50: '/content/drive/My Drive/glove.6B/glove.6B.50d.txt',\n",
        "    100: '/content/drive/My Drive/glove.6B/glove.6B.100d.txt',\n",
        "    200: '/content/drive/My Drive/glove.6B/glove.6B.200d.txt',\n",
        "    300: '/content/drive/My Drive/glove.6B/glove.6B.300d.txt'\n",
        "}\n",
        "# func to create embedding matrix\n",
        "def create_embedding_matrix(dim, word_index):\n",
        "    # load GloVe embeddings for specified dimension.\n",
        "    embedding_index = load_glove_embeddings(glove_paths[dim])\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, dim))\n",
        "    for word, i in word_index.items():\n",
        "        # corresponding GloVe vector\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "5LhN6yf50XGW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# func to calculate average embedding for each sequence\n",
        "def get_average_embedding(sequences, embedding_matrix):\n",
        "\n",
        "    embeddings = np.zeros((len(sequences), embedding_matrix.shape[1]))\n",
        "    for i, seq in enumerate(sequences):\n",
        "        # filter out zeros to find actual word indices\n",
        "        non_zero_elements = [idx for idx in seq if idx != 0]\n",
        "        if non_zero_elements:\n",
        "            # retrieve and average the embeddings for the non zero elements\n",
        "            seq_embeddings = np.array([embedding_matrix[idx] for idx in non_zero_elements])\n",
        "            embeddings[i] = np.mean(seq_embeddings, axis=0)\n",
        "    return embeddings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Iwp-Bjsb-oQL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for dim in glove_dimensions:\n",
        "    print(f\"Evaluating model with GloVe {dim}d embeddings...\")\n",
        "    # creating  embedding matrix for current dim\n",
        "    embedding_matrix = create_embedding_matrix(dim, word_index)\n",
        "    # converting training and evaluation sequences to their average embeddings\n",
        "    X_train_avg = get_average_embedding(X_train_pad, embedding_matrix)\n",
        "    X_eval_avg = get_average_embedding(X_eval_pad, embedding_matrix)\n",
        "\n",
        "    # initialize and train Logistic Regression model.\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_avg, y_train)\n",
        "\n",
        "    # predict and evaluate the model\n",
        "    y_pred = model.predict(X_eval_avg)\n",
        "\n",
        "\n",
        "    print(f\"Results for GloVe {dim}d embeddings:\")\n",
        "    print(f\"F1 Score: {f1_score(y_eval, y_pred)}\")\n",
        "    print(f\"Precision: {precision_score(y_eval, y_pred)}\")\n",
        "    print(f\"Recall: {recall_score(y_eval, y_pred)}\\n\")\n"
      ],
      "metadata": {
        "id": "s1I5IEyf-ri4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb159664-0d83-4e03-d92c-df4b3c6bdbc0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model with GloVe 50d embeddings...\n",
            "Results for GloVe 50d embeddings:\n",
            "F1 Score: 0.6027554535017221\n",
            "Precision: 0.6307569082899479\n",
            "Recall: 0.5771344814950531\n",
            "\n",
            "Evaluating model with GloVe 100d embeddings...\n",
            "Results for GloVe 100d embeddings:\n",
            "F1 Score: 0.6162347560975611\n",
            "Precision: 0.6419213973799127\n",
            "Recall: 0.5925247343349213\n",
            "\n",
            "Evaluating model with GloVe 200d embeddings...\n",
            "Results for GloVe 200d embeddings:\n",
            "F1 Score: 0.6387818041634541\n",
            "Precision: 0.6738511590077267\n",
            "Recall: 0.6071821179919384\n",
            "\n",
            "Evaluating model with GloVe 300d embeddings...\n",
            "Results for GloVe 300d embeddings:\n",
            "F1 Score: 0.6594427244582043\n",
            "Precision: 0.6986469864698647\n",
            "Recall: 0.6244045437889337\n",
            "\n"
          ]
        }
      ]
    }
  ]
}